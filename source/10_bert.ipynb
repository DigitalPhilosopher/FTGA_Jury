{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "     \n",
    "\n",
    "DATA = \"./data/preprocessed.json\"\n",
    "MODEL = \"bert-base-uncased\"\n",
    "\n",
    "RESPONSE_1 = \"1. Your company/vision/mission in a tweet ! *\"\n",
    "RESPONSE_2 = \"2. Which sector/subsector of Fintech do you operate in?\"\n",
    "RESPONSE_3 = \"3. SStage of company *\"\n",
    "RESPONSE_4 = \"4. Which problem does your company solve? *\"\n",
    "RESPONSE_5 = \"5. What sparked the founder(s) to set up the company? *\"\n",
    "RESPONSE_6 = \"6. Please describe the business model of your company, including the main sources of revenue. *\"\n",
    "RESPONSE_7 = \"7. What is your USP versus traditional competitors or those in the digital world? *\"\n",
    "RESPONSE_8 = \"8. Which target group(s) in which markets do you address primarily? *\"\n",
    "RESPONSE_9 = \"9. Please share any other office locations, you might have!\"\n",
    "RESPONSE_10 = \"10. Are you a German startup?\"\n",
    "RESPONSE_11 = \"11. If you are a foreign startup: Do you have clients in Germany? How many? In B2C/B2B?\"\n",
    "RESPONSE_12 = \"12. Please provide a brief competition landscape for your core markets. *\"\n",
    "RESPONSE_13 = \"13. Did you pivot in the past and why?\"\n",
    "RESPONSE_14 = \"14. Which business area in the last year consumed most of your management attention? *\"\n",
    "RESPONSE_15 = \"15. What protects your business from new market entrants or copycats? *\"\n",
    "RESPONSE_16 = \"16. After your next financing round, will the founders jointly hold above or below 50% of the\"\n",
    "RESPONSE_17 = \"17. Anything else you would want to add or let us know?\"\n",
    "RESPONSE_18 = \"18. How do you integrate ESG standards in your business model? *\"\n",
    "RESPONSE_19 = \"19. When scaling the company, how do you ensure that the corporate culture does not suffer and\"\n",
    "RESPONSE_20 = \"20. Please upload your startup logo. *\"\n",
    "RESPONSE_21 = \"21. Did your firm apply to FTGA before? If yes, which year? *\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(DATA)\n",
    "df = df[['Filename', RESPONSE_1, 'label']]\n",
    "df[RESPONSE_1] = df[RESPONSE_1].str.lower()\n",
    "\n",
    "text = df[RESPONSE_1].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL, do_lower_case=True)\n",
    "# Bestimmung der Maximallänge der Sätze, um Platz zu sparen\n",
    "max_len = max([len(tokenizer.encode(t, add_special_tokens=True)) for t in text])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Jetzt alle Sätze tokenisieren und IDs merken\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,    # '[CLS]' und '[SEP]'\n",
    "                        truncation = True,\n",
    "                        max_length=128,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  # Attention-Masks erzeugen\n",
    "                        return_tensors = 'pt',         # pytorch-Tensoren als Ergebnis\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Python-Listen in Tensoren wandeln\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Headline, Tokenisierung und IDs anzeigen\n",
    "print(text[0])\n",
    "print(tokenizer.tokenize(text[0]))\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Wir arbeiten ab jetzt nur noch mit dem Input-Tensor, der Attention Mask und den Labeln\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# wir nutzen einen 3:1-Split für Training und Validierung\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# die BERT-Autoren empfehlen für Finetuning Batch-Sizes von 16 oder 32\n",
    "batch_size = 16\n",
    "\n",
    "# DataLoader fÃ¼r die beiden Datensets erzeugen (man könnte auch RandomSampler verwenden)\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# das Modell muss zum Tokenizer passen!\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL, \n",
    "    num_labels = 7, # wir haben nur gut oder shlecht\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False # wir benÃ¶tigen keine Embeddings\n",
    ")\n",
    "# hier evtl. model.cpu() einsetzen\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimierer auswählen, AdamW ist Standard\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# vier Epochen, das kann justiert werden\n",
    "epochs = 32\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Accuracy berechnen\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU %s\" % torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU :-(\")\n",
    "\n",
    "# Statistik fÃ¼r das Training\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in trange(epochs, desc=\"Epoche\"):\n",
    "    # akkumulierter Loss für diese Epoche\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Modell in Trainingsmodus stellen\n",
    "    model.train()\n",
    "\n",
    "    # Trainig pro Batch\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # Daten entpacken und in device-Format wandeln\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Gradienten löschen\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Vorwärts-Auswertung (Trainingsdaten vorhersagen)\n",
    "        res = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Loss berechnen und akkumulieren\n",
    "        total_train_loss += res.loss.item()\n",
    "\n",
    "        # Rückwärts-Auswertung, um Gradienten zu bestimmen\n",
    "        res.loss.backward()\n",
    "\n",
    "        # Gradient beschrÃ¤nken wegen Exploding Gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Parameter und Lernrate aktualisieren\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "      \n",
    "\n",
    "    # Modell in Vorhersage-Modus umstellen\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Eine Epoche validieren\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validierung\"):\n",
    "        # jetzt die Validierungs-Daten entpacken\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Rückwärts-Auswertung wird nicht benötigt, daher auch kein Gradient\n",
    "        with torch.no_grad():        \n",
    "            # Vorhersage durchfÃ¼hren\n",
    "            res = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Loss akkumulieren\n",
    "        total_eval_loss += res.loss.item()\n",
    "\n",
    "        # Vorhersagedaten in CPU-Format wandeln, um Accuracy berechnen zu können\n",
    "        logits = res.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Accuracy für die Verifikation\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    tqdm.write(\"Accuracy: %f\" % avg_val_accuracy)\n",
    "\n",
    "    # Loss über alle Batches\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    tqdm.write(\"Validation loss %f\" % avg_val_loss)\n",
    "\n",
    "    # Statistik speichern für Auswertung\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validierung Loss': avg_val_loss,\n",
    "            'Accuracy': avg_val_accuracy\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_stats.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
